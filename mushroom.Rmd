---
title: 'STA 663: Graduate Project'
author: "Brian N. White"
date: "4/19/2020"
output: pdf_document
---
### Summary

For this project I 

### The Data

The data I used for the project can found via the following link: 

https://www.kaggle.com/uciml/mushroom-classification

This data set consists of 8124 observations on 23 variables. Each observation is a one of 23 species of gilded mushroom from the Agaricus and Lepiota Family. The variable 'class' tells us whether or not a mushroom is poisonous or not. The remaining 22 variables correspond morphological and ecological characteristics of mushrooms. Note that every variable in this data set is categorical. This information was simulated using the 1981 edition of The Audubon Society Field Guide to North American Mushrooms.

### Research Question

Can one predict whether or not a mushroom is poisonous given morphological and ecological data?

### Model Fitting

I began by loading the data and necessary packages.
```{r, load libraries and data message=FALSE}
library("xgboost")
library("rsample")
library("ranger")
library("tidymodels")

mushroom=read.csv("data/mushrooms.csv")
```

Next, I split the data into a training set and a test set. In addition, I prepped the training data for six-fold cross-validation.

```{r create training and testing data}
library(rsample)

set.seed(7)

mush_split=initial_split(mushroom, prop=0.7)

mush_train=training(mush_split)

mush_test=testing(mush_split)

mush_cv=vfold_cv(mush_train, v=6)
```

I then specified the model and tuned the parameter 'trees'. The optimal number of trees was found to be 50.

```{r tune random forest, cache=TRUE}
#random forest specification for tuning
rf_spec=rand_forest(mode="classification",
                    mtry=floor(sqrt(7)),
                               trees=tune())  %>%
  set_engine("ranger",
             importance="impurity")

#grid of 'tree' values to be consdiered in the tuning process
grid=expand.grid(trees=c(10, 25, 50, 100, 200, 300))

#the random forest is tuned
mush_model=tune_grid(rf_spec,
                     class~odor + habitat + gill.size + gill.color + population + ring.number + ring.type,
                     grid=grid,
                     resamples=mush_cv,
                     metrics=metric_set(gain_capture, accuracy))

#the optimal number of trees, with respect to the gini index, is determined
best_rf <- mush_model %>%
  select_best(metric = "gain_capture") %>%
  pull()
```

Last, I fit my tuned model on the training data set.
```{r fit final model}
#random forest specificatino with optimal number of trees
rf_final=rand_forest(mode="classification",
                    mtry=floor(sqrt(7)),
                               trees=best_rf) %>%
  set_engine("ranger",
             importance="impurity")

#the model is fit to the training data
mush_final=fit(rf_final, 
               class~odor + habitat + gill.size + gill.color + population + ring.number + ring.type,
               data=mush_train)
```

### Results

I then used my test data to evaluate the model's performance. The confusion matrix below gives a concise visual summary of its performance.

```{r plot confusion matrix}
#generates the confustion matrix for the model on the test data
mush_final %>%
  predict(new_data = mush_test) %>%
  bind_cols(mush_test) %>%
  conf_mat(truth = class, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

For completeness, I investigated the importance of each variable to the model. In the plot below, 'importance' corresponds to the total amount that the Gini Index is decreased by splits of a given predictor, averaged over the 50 trees.
```{r variable importance}

#variable importance
var_imp=ranger::importance(mush_final$fit)

#data frame created for use in plot
var_imp_df <- data.frame(
  variable = names(var_imp),
  importance = var_imp
  )

#plot of variable importance in the model w.r.t. the Gini index
var_imp_df %>%
ggplot(aes(x = variable, y = importance)) +
  geom_col() + 
  coord_flip()
```
